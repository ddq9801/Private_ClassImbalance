{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SVHN_PATE_Pytorch_eps5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHDbjt0HsVr_"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVV5GjdbD3DD"
      },
      "source": [
        "# !pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!lshw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeN7HLcYS-ZO"
      },
      "source": [
        "!pip install opacus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfwTH8FGUZS7"
      },
      "source": [
        "# !pip install torch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc04JdDwC72b"
      },
      "source": [
        "# !pip uninstall torchvision\n",
        "# !pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD1DlJoASIKN"
      },
      "source": [
        "!pip install syft==0.2.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRfhJ9Gbs5GQ"
      },
      "source": [
        "#!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n56W6wKL6E9"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "%load_ext tensorboard\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchsummary import summary\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Check assigned GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# set manual seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paQ6pqygsfdO"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tjnSDCthsBT"
      },
      "source": [
        "# !pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0akmmh3atx4_"
      },
      "source": [
        "# !pip install syft==0.2.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmGPZ0urNBG"
      },
      "source": [
        "!gsutil cp -r \"/gdrive/My Drive/SVHN3_1.zip\" \"../SVHN3.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW98msCHrM-E"
      },
      "source": [
        "!unzip \"../SVHN3.zip\" -d \"../\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_A8DuGHrM7b"
      },
      "source": [
        "!gsutil cp \"/gdrive/My Drive/MNIST_train.csv\" \"../MNIST3_train.csv\"\n",
        "!gsutil cp \"/gdrive/My Drive/MNIST_test.csv\" \"../MNIST3_test.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H58n8dT6VdZH"
      },
      "source": [
        "!ls \"../\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18kd656AsPuQ"
      },
      "source": [
        "# PREPROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Z6CnfnRFR4"
      },
      "source": [
        "new_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNJTK-lzRYOu"
      },
      "source": [
        "l = [\"Paths\", \"Labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJYr1sGRYMT"
      },
      "source": [
        "new_dict = dict(zip(new_dict, l))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4dsKIkSRYJl"
      },
      "source": [
        "new_dict.update(dict(zip(new_dict, l)))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMwX3uJIRYGk"
      },
      "source": [
        "pathlist = []\n",
        "label = []\n",
        "\n",
        "for file in os.listdir(\"../SVHN3_1/train/\"):\n",
        "  for n in os.listdir(\"../SVHN3_1/train/\"+file):\n",
        "    pathlist.append(\"../SVHN3_1/train/\"+file+\"/\"+n)\n",
        "    label.append(int(file))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sN7bkdPRYC8"
      },
      "source": [
        "len(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jalounBuRYAy"
      },
      "source": [
        "len(pathlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk9rv04pRX-p"
      },
      "source": [
        "data = {\n",
        "   'Paths': [],\n",
        "   'Labels': []\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Paths'] = pathlist\n",
        "df['Labels'] = label\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzzlK6AJU1xj"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcuWVIzhRX7e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv('../gdrive/My Drive/SVHN3_train.csv')\n",
        "df.to_csv('../SVHN3_train.csv')\n",
        "#files.download('../gdrive/My Drive/.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-xZtBtqs_HK"
      },
      "source": [
        "new_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPhrnX6s_HM"
      },
      "source": [
        "l = [\"Paths\", \"Labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9cPZiqGs_HN"
      },
      "source": [
        "new_dict = dict(zip(new_dict, l))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFkSeEoNs_HQ"
      },
      "source": [
        "new_dict.update(dict(zip(new_dict, l)))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF8wrkDDs_HR"
      },
      "source": [
        "pathlist = []\n",
        "label = []\n",
        "\n",
        "for file in os.listdir(\"../SVHN3_1/test/\"):\n",
        "  for n in os.listdir(\"../SVHN3_1/test/\"+file):\n",
        "    pathlist.append(\"../SVHN3_1/test/\"+file+\"/\"+n)\n",
        "    label.append(int(file))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B745h47s_HS"
      },
      "source": [
        "len(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9O4L2WDs_HT"
      },
      "source": [
        "len(pathlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXtg3UVRs_HU"
      },
      "source": [
        "data = {\n",
        "   'Paths': [],\n",
        "   'Labels': []\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Paths'] = pathlist\n",
        "df['Labels'] = label\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNo6ZkUds_HV"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGqLkL2vs_HV"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv('../gdrive/My Drive/SVHN3_test.csv')\n",
        "df.to_csv('../SVHN3_test.csv')\n",
        "#files.download('../gdrive/My Drive/.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKfOIE5nrMnl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi8c7Cf7rMlA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GITRHd_Uk0N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSGWOTc0V6eW"
      },
      "source": [
        "# START"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6fg1QEnstRM"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     \"\"\"Class used to initialize model of student/teacher\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "\n",
        "#         super(Model, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "#         self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "#         self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "#         self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, 2, 2)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, 2, 2)\n",
        "#         x = x.view(-1, 4 * 4 * 50)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2ynIZkCA_AM"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# class Model(nn.Module):\n",
        "#     \"\"\"Class used to initialize model of student/teacher\"\"\"\n",
        "#     def __init__(self):\n",
        "#         super(Model, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 96, 3, 1)\n",
        "#         self.conv2 = nn.Conv2d(96, 96, 3, 1)\n",
        "#         self.conv3 = nn.Conv2d(96, 96, 3, 1)\n",
        "#         self.conv4 = nn.Conv2d(96, 192, 3, 1)\n",
        "#         self.conv5 = nn.Conv2d(192, 192, 3, 1)\n",
        "#         self.conv6 = nn.Conv2d(192, 192, 3, 1)\n",
        "#         self.conv7 = nn.Conv2d(192, 192, 5, 1)\n",
        "#         self.fc1 = nn.Linear(2 * 2 * 192, 500)\n",
        "#         self.fc2 = nn.Linear(500, 10)\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = F.relu(self.conv5(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = F.relu(self.conv6(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         x = F.relu(self.conv7(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         # print(x.shape)\n",
        "#         x = x.view(-1, 2 * 2 * 192)\n",
        "#         # print(x.shape)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqJ446Jy3txL"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     \"\"\"Class used to initialize model of student/teacher\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "\n",
        "#         super(Model, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 96, 3, 1)\n",
        "#         self.conv2 = nn.Conv2d(96, 96, 3, 1)\n",
        "#         self.conv3 = nn.Conv2d(96, 96, 3, 1)\n",
        "#         self.bn1 = nn.BatchNorm2d(96)\n",
        "#         self.conv4 = nn.Conv2d(96, 192, 3, 1)\n",
        "#         self.conv5 = nn.Conv2d(192, 192, 3, 1)\n",
        "#         self.conv6 = nn.Conv2d(192, 192, 3, 1)\n",
        "#         self.conv7 = nn.Conv2d(192, 192, 5, 1)\n",
        "#         self.bn2 = nn.BatchNorm2d(192)\n",
        "#         self.fc1 = nn.Linear(2 * 2 * 192, 500)\n",
        "#         self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn1(x)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn1(x)\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn1(x)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn2(x)\n",
        "#         x = F.relu(self.conv5(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn2(x)\n",
        "#         x = F.relu(self.conv6(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn2(x)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         x = F.relu(self.conv7(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         x = self.bn2(x)\n",
        "#         # print(x.shape)\n",
        "#         x = x.view(-1, 2 * 2 * 192)\n",
        "#         # print(x.shape)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsHJN1Bptv0F"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfuUkjP3tSM7"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import opacus \n",
        "from opacus import PrivacyEngine\n",
        "\n",
        "class Student:\n",
        "    \"\"\"Implementation of Student models\n",
        "       The student model is trained from the public data labelled by teacher ensembles.\n",
        "       The teacher ensembles were trained using sensitive data. The student model is further\n",
        "       used to make predictions on public data.\n",
        "       Args:\n",
        "           args[Arguments obj]: Object of arguments class used to control hyperparameters\n",
        "           model[torch model]: Model of Student \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, model):\n",
        "\n",
        "        self.args = args\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Function which accepts unlabelled public data and labels it using \n",
        "           teacher's model.\n",
        "           Args:\n",
        "               model[torch model]: Teachers model\n",
        "               data [torch tensor]: Public unlabelled data\n",
        "           Returns:\n",
        "               dataset[Torch tensor]: Labelled public dataset\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.max(self.model(data), 1)[1]\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"Function to train the student model.\n",
        "           Args:\n",
        "               dataset[torch dataset]: Dataset using which model is trained.\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(0, self.args.student_epochs):\n",
        "            self.loop_body(dataset, epoch)\n",
        "\n",
        "    def loop_body(self, dataset, epoch):\n",
        "        \"\"\"Body of the training loop.\n",
        "           Args:\n",
        "               dataset: dataset which is used to train the model.\n",
        "               epoch: Epoch for which the model is being trained.\n",
        "        \"\"\"\n",
        "\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=self.args.lr, momentum=self.args.momentum)\n",
        "        iters = 0\n",
        "        loss = 0.0\n",
        "        batch_size = sample_size = 128\n",
        "        criterion=nn.CrossEntropyLoss()\n",
        "        \n",
        "        # if epoch == 1:\n",
        "        #   privacy_engine = PrivacyEngine(\n",
        "        #       model,\n",
        "        #       batch_size = batch_size,\n",
        "        #       sample_size = sample_size,\n",
        "        #       alphas=[10, 100],\n",
        "        #       noise_multiplier= 12,\n",
        "        #       max_grad_norm = 1.0,\n",
        "        #       target_delta = 1e-22,\n",
        "        #   )\n",
        "        #   privacy_engine.attach(optimizer)\n",
        "        self.model = self.model.cuda()\n",
        "        for (data, target) in dataset:\n",
        "            data, target = data.cuda(), target.cuda() \n",
        "            optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            iters += 1\n",
        "        # Print loss by making using of log intervals\n",
        "        print(\"\\n\")\n",
        "        print(f\"EPOCH {epoch}\\t\\tLoss: {loss.item()}\")\n",
        "        # print(epoch)\n",
        "        # print(\"\\n\")\n",
        "        # print(\"Loss\")\n",
        "        # print(loss.item())\n",
        "\n",
        "    def save_model(self):\n",
        "        torch.save(self.model.state_dict(), \"Models/\" + \"student_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcLavDFCtd-B"
      },
      "source": [
        "def split(dataset, batch_size, split=0.2):\n",
        "    \"\"\"Splits the given dataset into training/validation.\n",
        "       Args:\n",
        "           dataset[torch dataloader]: Dataset which has to be split\n",
        "           batch_size[int]: Batch size\n",
        "           split[float]: Indicates ratio of validation samples\n",
        "       Returns:\n",
        "           train_set[list]: Training set\n",
        "           val_set[list]: Validation set\n",
        "    \"\"\"\n",
        "\n",
        "    index = 0\n",
        "    length = len(dataset)\n",
        "\n",
        "    train_set = []\n",
        "    val_set = []\n",
        "\n",
        "    for data, target in dataset:\n",
        "        if index <= (length * split):\n",
        "            train_set.append([data, target])\n",
        "        else:\n",
        "            val_set.append([data, target])\n",
        "\n",
        "        index += 1\n",
        "\n",
        "    return train_set, val_set\n",
        "\n",
        "\n",
        "def accuracy(predictions, dataset):\n",
        "    \"\"\"Evaluates accuracy for given set of predictions and true labels.\n",
        "       Args:\n",
        "           predictions[torch tensor]: predictions made by classifier.\n",
        "           labels[torch tensor]: true labels of the dataset.\n",
        "       Returns:\n",
        "           accuracy[float]: accuracy of classifier.\n",
        "    \"\"\"\n",
        "    # predictions, dataset = predictions.cuda(), dataset.cuda()\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    for j in range(0, len(dataset)):\n",
        "        correct += (predictions[j].cpu().long() == dataset[j].cpu().long()).sum().item()\n",
        "        total += len(dataset[j])\n",
        "\n",
        "    return (correct / total) * 100\n",
        "\n",
        "\n",
        "def plot(x, y):\n",
        "    \"\"\"Plots a graph of given x and y.\n",
        "       Args:\n",
        "           \n",
        "           x:\n",
        "           y:\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def histogram(x, y):\n",
        "    \"\"\"Plots a histogram for corresponding x and y:\n",
        "       Args:\n",
        "           \n",
        "           x:\n",
        "           y:\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmCDUfAttWK6"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions.laplace import Laplace\n",
        "#from util import accuracy\n",
        "from syft.frameworks.torch.dp import pate\n",
        "\n",
        "class Teacher:\n",
        "    \"\"\"Implementation of teacher models.\n",
        "       Teacher models are ensemble of models which learns directly disjoint splits of the sensitive data\n",
        "       The ensemble of teachers are further used to label unlabelled public data on which the student is \n",
        "       trained. \n",
        "       Args:\"\n",
        "           args[Arguments object]: An object of Arguments class with required hyperparameters\n",
        "           n_teachers[int]: Number of teachers\n",
        "           epochs[int]: Number of epochs to train each model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, model, n_teachers=1, epsilon=0.5):\n",
        "\n",
        "        self.n_teachers = n_teachers\n",
        "        self.model = model\n",
        "        self.models = {}\n",
        "        self.args = args\n",
        "        self.init_models()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def init_models(self):\n",
        "        \"\"\"Initialize teacher models according to number of required teachers\"\"\"\n",
        "\n",
        "        name = \"model_\"\n",
        "        for index in range(0, self.n_teachers):\n",
        "\n",
        "            model = self.model()\n",
        "            self.models[name + str(index)] = model\n",
        "\n",
        "    def addnoise(self, x):\n",
        "        \"\"\"Adds Laplacian noise to histogram of counts\n",
        "           Args:\n",
        "                counts[torch tensor]: Histogram counts\n",
        "                epsilon[integer]:Amount of Noise\n",
        "           Returns:\n",
        "                counts[torch tensor]: Noisy histogram of counts\n",
        "        \"\"\"\n",
        "\n",
        "        m = Laplace(torch.tensor([0.0]), torch.tensor([self.epsilon]))\n",
        "        count = x + m.sample()\n",
        "\n",
        "        return count\n",
        "\n",
        "    def split(self, dataset):\n",
        "        \"\"\"Function to split the dataset into non-overlapping subsets of the data\n",
        "           Args:\n",
        "               dataset[torch tensor]: The dataset in the form of (image,label)\n",
        "           Returns:\n",
        "               split: Split of dataset\n",
        "        \"\"\"\n",
        "\n",
        "        ratio = int(len(dataset) / self.n_teachers)\n",
        "        iters = 0\n",
        "        index = 0\n",
        "        split = []\n",
        "        last_batch = ratio * self.n_teachers\n",
        "\n",
        "        for teacher in range(0, self.n_teachers):\n",
        "\n",
        "            split.append([])\n",
        "\n",
        "        for (data, target) in dataset:\n",
        "            if (iters) % ratio == 0 and iters != 0:\n",
        "\n",
        "                index += 1\n",
        "\n",
        "            split[index].append([data, target])\n",
        "            iters += 1\n",
        "\n",
        "            if iters == last_batch:\n",
        "                return split\n",
        "\n",
        "        return split\n",
        "\n",
        "    def train(self, dataset):\n",
        "        \"\"\"Function to train all teacher models.\n",
        "           Args:\n",
        "                dataset[torch tensor]: Dataset used to train teachers in format (image,label)\n",
        "        \"\"\"\n",
        "\n",
        "        split = self.split(dataset)\n",
        "        # print(self.args.epochs)\n",
        "        \n",
        "        for epoch in range(1, self.args.epochs + 1):\n",
        "            \n",
        "            print(\"EPOCH: \", epoch)\n",
        "            index = 0\n",
        "            for model_name in self.models:\n",
        "\n",
        "                # print(\"TRAINING \", model_name)\n",
        "                \n",
        "                self.loop_body(split[index], model_name, epoch)\n",
        "                index += 1\n",
        "\n",
        "    def loop_body(self, split, model_name, epoch):\n",
        "        \"\"\"Body of the training loop.\n",
        "           Args:\n",
        "               split: Split of the dataset which the model has to train.\n",
        "               model_name: Name of the model.\n",
        "               epoch: Epoch for which the model is being trained.\n",
        "        \"\"\"\n",
        "\n",
        "        model = self.models[model_name].cuda()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=self.args.lr, momentum=self.args.momentum)\n",
        "        criterion=nn.CrossEntropyLoss()\n",
        "        iters = 0\n",
        "        loss = 0.0\n",
        "        for (data, target) in split:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            iters += 1\n",
        "        # Print loss by making using of log intervals\n",
        "        print(f\"Training {model_name}\\t\\tLoss: {loss.item()}\")\n",
        "        # print(loss.item())\n",
        "\n",
        "    def aggregate(self, model_votes, batch_size):\n",
        "        \"\"\"Aggregate model output into a single tensor of votes of all models.\n",
        "           Args:\n",
        "                votes: Model output\n",
        "                n_dataset: Number of datapoints\n",
        "           Returns:\n",
        "                counts: Torch tensor with counts across all models    \n",
        "           \"\"\"\n",
        "\n",
        "        counts = torch.zeros([batch_size, 10])\n",
        "        model_counts = torch.zeros([self.args.n_teachers, batch_size])\n",
        "        model_index = 0\n",
        "\n",
        "        for model in model_votes:\n",
        "\n",
        "            index = 0\n",
        "\n",
        "            for tensor in model_votes[model]:\n",
        "                for val in tensor:\n",
        "\n",
        "                    counts[index][val] += 1\n",
        "                    model_counts[model_index][index] = val\n",
        "                    index += 1\n",
        "\n",
        "            model_index += 1\n",
        "\n",
        "        return counts, model_counts\n",
        "\n",
        "    def save_models(self):\n",
        "        no = 0\n",
        "        for model in self.models:\n",
        "\n",
        "            torch.save(self.models[model].state_dict(), \"/gdrive/My Drive/PATE_Teacher/\" + model)\n",
        "            no += 1\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"MODELS SAVED\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    def load_models(self):\n",
        "\n",
        "        path_name = \"model_\"\n",
        "\n",
        "        for i in range(0, self.args.n_teachers):\n",
        "\n",
        "            modelA = self.model()\n",
        "            self.models[path_name + str(i)] = torch.load(\"/gdrive/My Drive/PATE_Teacher/\" + path_name + str(i))\n",
        "            self.models[path_name + str(i)] = modelA.load_state_dict()\n",
        "\n",
        "    def analyze(self, preds, indices, moments=8):\n",
        "\n",
        "        datadepeps, dataindeps = pate.perform_analysis_torch(\n",
        "            preds, indices, noise_eps = 0.45 * 1e-2, delta= 1e-6, moments=moments, beta=0.09\n",
        "        ) # increase noise_eps and fix delta\n",
        "        return datadepeps, dataindeps\n",
        "\n",
        "    def predict(self, data):\n",
        "        \"\"\"Make predictions using Noisy-max using Laplace mechanism.\n",
        "           Args:\n",
        "                data: Data for which predictions are to be made\n",
        "           Returns:\n",
        "                predictions: Predictions for the data\n",
        "        \"\"\"\n",
        "\n",
        "        model_predictions = {}\n",
        "\n",
        "        for model in self.models:\n",
        "            self.models[model] = self.models[model].cuda()\n",
        "            out = []\n",
        "            output = self.models[model](data)\n",
        "            output = output.max(dim=1)[1]\n",
        "            out.append(output)\n",
        "\n",
        "            model_predictions[model] = out\n",
        "\n",
        "        counts, model_counts = self.aggregate(model_predictions, len(data))\n",
        "        counts = counts.apply_(self.addnoise)\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for batch in counts:\n",
        "\n",
        "            predictions.append(torch.tensor(batch.max(dim=0)[1].long()).clone().detach())\n",
        "\n",
        "        output = {\"predictions\": predictions, \"counts\": counts, \"model_counts\": model_counts}\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtxbortZFDmg"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_42EiqFgC9s2"
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dTtS8Iq4KVu"
      },
      "source": [
        "# !sudo apt upgrade python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR7QWIZUCSDT"
      },
      "source": [
        "# !sudo apt-get update "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHQh0FZRB4a3"
      },
      "source": [
        "model = ResNet18()\n",
        "summary(model.cuda(), (3,32,32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqnhIQCbta59"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_data(train, batch_size):\n",
        "    \"\"\"Helper function used to load the train/test data.\n",
        "       Args:\n",
        "           train[boolean]: Indicates whether its train/test data.\n",
        "           batch_size[int]: Batch size\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_mnist = datasets.MNIST(\n",
        "            \"../data\",\n",
        "            train=train,\n",
        "            download=True,\n",
        "            transform=transforms.Compose(\n",
        "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "            ),\n",
        "        )\n",
        "    classes, class_counts = np.unique(dataset_mnist.targets, return_counts=True)\n",
        "    nb_classes = len(classes)\n",
        "    if train == True:\n",
        "      # imbal_class_counts = [6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 500, 6000]\n",
        "\n",
        "      # # Get class indices\n",
        "      # class_indices = [np.where(np.array(dataset_mnist.targets) == i)[0] for i in range(nb_classes)]\n",
        "\n",
        "      # # Get imbalanced number of instances\n",
        "      # imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
        "      # imbal_class_indices = np.hstack(imbal_class_indices)\n",
        "      # #print(type(imbal_class_indices))\n",
        "      # #print(imbal_class_indices[0:2])\n",
        "\n",
        "\n",
        "      # # Set target and data to dataset\n",
        "      # dataset_mnist.targets = np.array(dataset_mnist.targets)[imbal_class_indices]\n",
        "      # dataset_mnist.data = dataset_mnist.data[imbal_class_indices]\n",
        "\n",
        "\n",
        "      data_dir = '/content/drive/My Drive/MNIST'\n",
        "\n",
        "      dataset_mnist = []\n",
        "\n",
        "      for label in os.listdir(data_dir):\n",
        "        for img in os.listdir(os.path.join(data_dir, label)):\n",
        "          dataset_mnist.append((img, label))\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset_mnist,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "class NoisyDataset(Dataset):\n",
        "    \"\"\"Dataset with targets predicted by ensemble of teachers.\n",
        "       Args:\n",
        "            dataloader (torch dataloader): The original torch dataloader.\n",
        "            model(torch model): Teacher model to make predictions.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataloader, predictionfn, transform=None):\n",
        "        self.dataloader = dataloader\n",
        "        self.predictionfn = predictionfn\n",
        "        self.transform = transform\n",
        "        self.noisy_data = self.process_data()\n",
        "\n",
        "    def process_data(self):\n",
        "        \"\"\"\n",
        "        Replaces original targets with targets predicted by ensemble of teachers.\n",
        "        Returns:\n",
        "            noisy_data[torch tensor]: Dataset with labels predicted by teachers\n",
        "            \n",
        "        \"\"\"\n",
        "\n",
        "        noisy_data = []\n",
        "        #self.predictionfn = self.predictionfn.cuda()\n",
        "        for data, _ in self.dataloader:\n",
        "            data=data.cuda()\n",
        "            noisy_data.append([data, torch.tensor(self.predictionfn(data)[\"predictions\"])])\n",
        "\n",
        "        return noisy_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataloader)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sample = self.noisy_data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tCvtrNQbcuS"
      },
      "source": [
        "# # len(test_loader.dataset.data)\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# train_loader = load_data(True, 32)\n",
        "\n",
        "\n",
        "# for (label, img) in train_loader:\n",
        "#   print(label)\n",
        "#   plt.imshow(img)\n",
        "#   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsYTkypWOmG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTi_ZuP_tiXu"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51YwR--s3_F"
      },
      "source": [
        "class CustomDatasetFromCsvData(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset example for reading data from csv\n",
        "        Args:\n",
        "            csv_path (string): path to csv file\n",
        "            height (int): image height\n",
        "            width (int): image width\n",
        "            transform: pytorch transforms for transforms and tensor conversion\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.paths = self.data['Paths']\n",
        "        self.labels = np.asarray(self.data['Labels'])\n",
        "        #self.height = height\n",
        "        #self.width = width\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        # Read each 784 pixels and reshape the 1D array ([784]) to 2D array ([28,28])\n",
        "        img_as_np = Image.open(self.paths[index])\n",
        "        #print(type(img_as_np))\n",
        "        img_as_np = np.asarray(img_as_np)\n",
        "        img_as_np = np.reshape(img_as_np, (32,32,3))\n",
        "        #img_as_np = np.asarray(self.data.iloc[index][1:]).reshape(28, 28).astype('uint8')\n",
        "        # Convert image from numpy array to PIL image, mode 'L' is for grayscale\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        #img_as_img = img_as_img.convert('L')\n",
        "        # Transform image to tensor\n",
        "        if self.transform is not None:\n",
        "            img_as_tensor = self.transform(img_as_img)\n",
        "        #print(type(img_as_tensor))\n",
        "        #img_as_tensor = Variable(img_as_tensor)\n",
        "        #single_image_label = Variable(torch.Tensor(single_image_label))\n",
        "        # Return image and the label\n",
        "        return (img_as_tensor, single_image_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.index)\n",
        "\n",
        "\n",
        "class ExampleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkQP8s8JgSiM"
      },
      "source": [
        "transforms_svhn = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbYalrrjs36_"
      },
      "source": [
        "#train_loader = DataLoader(CustomDataset('../gdrive/My Drive/MNIST/'), batch_size=32, shuffle=True)\n",
        "\n",
        "#transformations_train = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "#transformations_test = transforms.Compose([transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "PATH_TO_CIFAR3 = '../SVHN3_1'\n",
        "\n",
        "trainset = datasets.ImageFolder(root=PATH_TO_CIFAR3+'/train',transform=transforms_svhn)\n",
        "testset = datasets.ImageFolder(root=PATH_TO_CIFAR3+'/test',transform=transforms_svhn)\n",
        "#trainset = CustomDatasetFromCsvData(\"../SVHN3_train.csv\", transforms_svhn)\n",
        "#testset = CustomDatasetFromCsvData(\"../SVHN3_test.csv\", transforms_svhn)\n",
        "\n",
        "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "#testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FyUMBzfWObc"
      },
      "source": [
        "# train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=128, shuffle=True, drop_last = True)\n",
        "# test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=128, shuffle=True, drop_last = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0yiVv6AMEju"
      },
      "source": [
        "# for target, img in test_loader:\n",
        "#   print(target[0].shape)\n",
        "#   break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r65OemhU5NLB"
      },
      "source": [
        "# Required imports\n",
        "import torch\n",
        "#from Teacher import Teacher\n",
        "#from Model import Model\n",
        "#from data import load_data, NoisyDataset\n",
        "#from util import accuracy, split\n",
        "#from Student import Student\n",
        "import syft as sy\n",
        "from syft.frameworks.torch.dp import pate\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "class Arguments:\n",
        "    # Class used to set hyperparameters for the whole PATE implementation\n",
        "    def __init__(self):\n",
        "        self.batchsize = 128\n",
        "        self.test_batchsize = 128\n",
        "        self.epochs = 50\n",
        "        self.student_epochs = 30\n",
        "        self.lr = 0.05\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 30\n",
        "        self.n_teachers = 250\n",
        "        self.save_model = False\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "class_accuracies = []\n",
        "#train_loader = load_data(True, args.batchsize)\n",
        "#test_loader = load_data(False, args.test_batchsize)\n",
        "\n",
        "for i in range(1):\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=args.batchsize, shuffle=True, drop_last = True)\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=args.test_batchsize, shuffle=True, drop_last = True)\n",
        "\n",
        "\n",
        "  # Declare and train teachers on MNIST training data\n",
        "  teacher = Teacher(args, ResNet18, n_teachers=args.n_teachers)\n",
        "  teacher.train(train_loader)\n",
        "\n",
        "  # Evaluate Teacher accuracy\n",
        "  teacher_targets = []\n",
        "  predict = []\n",
        "\n",
        "  counts = []\n",
        "  original_targets = []\n",
        "\n",
        "  print(f'Iteration {i}\\n\\n')\n",
        "\n",
        "  for data, target in test_loader:\n",
        "    data,target=data.cuda(),target.cuda()\n",
        "    output = teacher.predict(data)\n",
        "\n",
        "    arr_target = []\n",
        "    teacher_targets.append(target)\n",
        "    original_targets.append(target)\n",
        "    predict.append(output[\"predictions\"])\n",
        "    counts.append(output[\"model_counts\"])\n",
        "    print(\"Accuracy: \", accuracy(torch.tensor(predict), teacher_targets))\n",
        "\n",
        "  print(\"Accuracy: \", accuracy(torch.tensor(predict), teacher_targets))\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Training Student\")\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # Split the test data further into training and validation data for student\n",
        "  train, val = split(test_loader, args.batchsize)\n",
        "\n",
        "  student = Student(args, ResNet18())\n",
        "\n",
        "  N = NoisyDataset(train, teacher.predict)\n",
        "  student.train(N)\n",
        "\n",
        "  results = []\n",
        "  targets = []\n",
        "\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "\n",
        "  for data, target in val:\n",
        "    data,target=data.cuda(),target.cuda()\n",
        "    predict_lol1 = student.predict(data)\n",
        "    correct += float((predict_lol1 == (target)).sum().item())\n",
        "    total += float(target.size(0))\n",
        "\n",
        "  print(\"Private Baseline: \", (correct / total) * 100)\n",
        "\n",
        "  counts_lol = torch.stack(counts).contiguous()\n",
        "\n",
        "  counts_lol = counts_lol.view(250, 128*74)\n",
        "  predict_lol = torch.tensor(predict).view(128*74)\n",
        "\n",
        "  data_dep_eps, data_ind_eps = teacher.analyze(counts_lol, predict_lol, moments=20)\n",
        "  print(f\"Dependent Epsilon: {data_dep_eps}\\nIndependent Epsilon: {data_ind_eps}\")\n",
        "\n",
        "  targets_1 = []\n",
        "  predict_1 = []\n",
        "\n",
        "  for data, target in val:\n",
        "    data,target=data.cuda(),target.cuda()\n",
        "    predict_1.append(student.predict(data))\n",
        "    targets_1.append(target)\n",
        "\n",
        "  ### Class-Wise Accuracy of the predictions on the base model\n",
        "\n",
        "  for i in range(len(targets_1)):\n",
        "      targets_1[i] = targets_1[i].cpu()\n",
        "      predict_1[i] = predict_1[i].cpu()\n",
        "\n",
        "  conf_mat=confusion_matrix(np.concatenate(targets_1), np.concatenate(predict_1))\n",
        "  print(conf_mat)\n",
        "  class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "  print(class_accuracy)\n",
        "  class_accuracies.append(class_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3o1p5Tn9TiG"
      },
      "source": [
        "class_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UEtm_9H9Tfd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1rU4V-p9TG-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WofO6kghqmtx"
      },
      "source": [
        "counts_lol = counts_lol.view(50, 10*950)\n",
        "predict_lol = torch.tensor(predict).view(10*950)\n",
        "\n",
        "data_dep_eps, data_ind_eps = teacher.analyze(counts_lol, predict_lol, moments=20)\n",
        "print(f\"Dependent Epsilon: {data_dep_eps}\\nIndependent Epsilon: {data_ind_eps}\")\n",
        "\n",
        "targets_1 = []\n",
        "predict_1 = []\n",
        "\n",
        "for data, target in val:\n",
        "  data,target=data.cuda(),target.cuda()\n",
        "  predict_1.append(student.predict(data))\n",
        "  targets_1.append(target)\n",
        "\n",
        "### Class-Wise Accuracy of the predictions on the base model\n",
        "\n",
        "for i in range(len(targets_1)):\n",
        "    targets_1[i] = targets_1[i].cpu()\n",
        "    predict_1[i] = predict_1[i].cpu()\n",
        "\n",
        "conf_mat=confusion_matrix(np.concatenate(targets_1), np.concatenate(predict_1))\n",
        "print(conf_mat)\n",
        "class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "print(class_accuracy)\n",
        "class_accuracies.append(class_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ofmdjKS7Ew-"
      },
      "source": [
        "class_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhyLjtZqyaNK"
      },
      "source": [
        "#from collections import Counter\n",
        "#c = Counter(predict_lol.tolist())\n",
        "#c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6BEZ7JrH2Kq"
      },
      "source": [
        "# counts_lol = torch.stack(counts).contiguous()\n",
        "# print(counts_lol.shape)\n",
        "#   # counts_lol = counts_lol.view(50, 24500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpElqn_dpbD2"
      },
      "source": [
        "# counts_lol = torch.stack(counts).contiguous()\n",
        "# # print(counts_lol.shape)\n",
        "# counts_lol = counts_lol.view(50, 24448)\n",
        "# predict_lol = torch.tensor(predict).view(24448)\n",
        "\n",
        "# print(counts_lol.shape, predict_lol.shape)\n",
        "\n",
        "# data_dep_eps, data_ind_eps = teacher.analyze(counts_lol, predict_lol, moments=20)\n",
        "# print(f\"Dependent Epsilon: {data_dep_eps}\\nIndependent Epsilon: {data_ind_eps}\")\n",
        "\n",
        "# targets_1 = []\n",
        "# predict_1 = []\n",
        "\n",
        "# for data, target in val:\n",
        "#   predict_1.append(student.predict(data))\n",
        "#   targets_1.append(target)\n",
        "\n",
        "# ### Class-Wise Accuracy of the predictions on the base model\n",
        "\n",
        "\n",
        "# conf_mat=confusion_matrix(np.concatenate(targets_1), np.concatenate(predict_1))\n",
        "# print(conf_mat)\n",
        "# class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "# print(class_accuracy)\n",
        "# class_accuracies.append(class_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-2VmBHy9bCs"
      },
      "source": [
        "# data_dep_eps, data_ind_eps = teacher.analyze(counts_lol, predict_lol, moments=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHRkSujgQXr9"
      },
      "source": [
        "# print(f\"Dependent Epsilon: {data_dep_eps}\\nIndependent Epsilon: {data_ind_eps}\")\n",
        "\n",
        "#   targets_1 = []\n",
        "#   predict_1 = []\n",
        "\n",
        "#   for data, target in val:\n",
        "#     predict_1.append(student.predict(data))\n",
        "#     targets_1.append(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-aT3o-IbcT9"
      },
      "source": [
        "# # Evaluate Teacher accuracy\n",
        "# teacher_targets = []\n",
        "# predict = []\n",
        "\n",
        "# counts = []\n",
        "# original_targets = []\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(dataset=mnist_test_data, batch_size=args.test_batchsize, shuffle=True, drop_last = True)\n",
        "\n",
        "# for data, target in test_loader:\n",
        "\n",
        "#     output = teacher.predict(data)\n",
        "\n",
        "#     arr_target = []\n",
        "#     teacher_targets.append(target)\n",
        "#     original_targets.append(target)\n",
        "#     predict.append(output[\"predictions\"])\n",
        "#     counts.append(output[\"model_counts\"])\n",
        "#     print(\"Accuracy: \", accuracy(torch.tensor(predict), teacher_targets))\n",
        "\n",
        "# print(\"Accuracy: \", accuracy(torch.tensor(predict), teacher_targets))\n",
        "\n",
        "# print(\"\\n\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "# print(\"Training Student\")\n",
        "\n",
        "# print(\"\\n\")\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Split the test data further into training and validation data for student\n",
        "# train, val = split(test_loader, args.batchsize)\n",
        "\n",
        "# student = Student(args, Model())\n",
        "# N = NoisyDataset(train, teacher.predict)\n",
        "# student.train(N)\n",
        "\n",
        "# results = []\n",
        "# targets = []\n",
        "\n",
        "# total = 0.0\n",
        "# correct = 0.0\n",
        "\n",
        "# for data, target in val:\n",
        "\n",
        "#     predict_lol1 = student.predict(data)\n",
        "#     correct += float((predict_lol1 == (target)).sum().item())\n",
        "#     total += float(target.size(0))\n",
        "\n",
        "# print(\"Private Baseline: \", (correct / total) * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joVjbAnqlmJR"
      },
      "source": [
        "# counts_lol = torch.stack(counts).contiguous()\n",
        "# # print(counts_lol.shape)\n",
        "# counts_lol = counts_lol.view(50, 9100)\n",
        "# predict_lol = torch.tensor(predict).view(9100)\n",
        "\n",
        "# data_dep_eps, data_ind_eps = teacher.analyze(counts_lol, predict_lol, moments= 20)\n",
        "# print(f\"Dependent Epsilon: {data_dep_eps}\\nIndependent Epsilon: {data_ind_eps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5WGsqNmx6r"
      },
      "source": [
        "# # Evaluate Teacher accuracy\n",
        "# teacher_targets = []\n",
        "# predict = []\n",
        "\n",
        "# counts = []\n",
        "# original_targets = []\n",
        "\n",
        "\n",
        "# for data, target in test_loader:\n",
        "\n",
        "#     output = teacher.predict(data)\n",
        "\n",
        "#     arr_target = []\n",
        "#     teacher_targets.append(target)\n",
        "#     original_targets.append(target)\n",
        "#     predict.append(output[\"predictions\"])\n",
        "#     counts.append(output[\"model_counts\"])\n",
        "#     print(\"Accuracy: \", accuracy(torch.tensor(predict), teacher_targets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x04rTrkjnkIu"
      },
      "source": [
        "# # Split the test data further into training and validation data for student\n",
        "# train, val = split(test_loader, args.batchsize)\n",
        "\n",
        "# student = Student(args, Model())\n",
        "# N = NoisyDataset(train, teacher.predict)\n",
        "# student.train(N)\n",
        "\n",
        "# results = []\n",
        "# targets = []\n",
        "\n",
        "# total = 0.0\n",
        "# correct = 0.0\n",
        "\n",
        "# for data, target in val:\n",
        "\n",
        "#     predict_lol1 = student.predict(data)\n",
        "#     correct += float((predict_lol1 == (target)).sum().item())\n",
        "#     total += float(target.size(0))\n",
        "\n",
        "# print(\"Private Baseline: \", (correct / total) * 100)\n",
        "\n",
        "# counts_lol = torch.stack(counts).contiguous().view(50, 9100)\n",
        "# predict_lol = torch.tensor(predict).view(9100)\n",
        "\n",
        "# data_dep_eps, data_ind_eps = teacher.analyze(counts_lol, predict_lol, moments= 20)\n",
        "# print(f\"Dependent Epsilon: {data_dep_eps}\\nIndependent Epsilon: {data_ind_eps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ5HSFsP5UtX"
      },
      "source": [
        "counts_lol.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xj5FYH8Ckc_"
      },
      "source": [
        "predict_lol.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TD1z2R5Cnwi"
      },
      "source": [
        "# targets_1 = []\n",
        "# predict_1 = []\n",
        "\n",
        "# for data, target in val:\n",
        "#   predict_1.append(student.predict(data))\n",
        "#   targets_1.append(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZtLoKkb_Ldd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otDrcHJf_LgE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YH7rUBw_N_d"
      },
      "source": [
        "# ### Class-Wise Accuracy of the predictions on the base model\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# conf_mat=confusion_matrix(np.concatenate(targets_1), np.concatenate(predict_1))\n",
        "# print(conf_mat)\n",
        "# class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "# print(class_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZRXSCl25S2B"
      },
      "source": [
        "for acc in class_accuracies:\n",
        "  print(acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}