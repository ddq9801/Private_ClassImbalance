{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVHN_DPSGD_eps5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6dxeQuCb8mc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfNw1yT1JDF"
      },
      "source": [
        "#!pip install -q tensorflow_privacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R4cunRWyDXN"
      },
      "source": [
        "!pip install opacus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prLQHiEPoXUT"
      },
      "source": [
        "!ls \"../\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Appzq0U8cGBt"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive', force_remount = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFsdeHKXcWkc"
      },
      "source": [
        "!gsutil cp -r \"/gdrive/My Drive/SVHN3_1.zip\" \"../SVHN3_1.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F-Vhw9zMiJh"
      },
      "source": [
        "!unzip \"../SVHN3_1.zip\" -d \"../\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca3c0xbNMxXj"
      },
      "source": [
        "!ls -lrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L4e5-xTcN9f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm as tqdm\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import copy\n",
        "from torchsummary import summary\n",
        "from PIL import Image\n",
        "\n",
        "import opacus\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 42\n",
        "best_model = None\n",
        "best_loss = 0.\n",
        "best_test_loss = 0.\n",
        "best_test_acc = 0.\n",
        "best_pred_labels = []\n",
        "true_labels = []\n",
        "\n",
        "pred_labels = []\n",
        "test_acc = 0.\n",
        "test_loss = 0.\n",
        "\n",
        "# device = torch.device('cuda:0')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvb5zdY17TW9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18kd656AsPuQ"
      },
      "source": [
        "# PREPROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Z6CnfnRFR4"
      },
      "source": [
        "new_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNJTK-lzRYOu"
      },
      "source": [
        "l = [\"Paths\", \"Labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJYr1sGRYMT"
      },
      "source": [
        "new_dict = dict(zip(new_dict, l))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4dsKIkSRYJl"
      },
      "source": [
        "new_dict.update(dict(zip(new_dict, l)))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMwX3uJIRYGk"
      },
      "source": [
        "pathlist = []\n",
        "label = []\n",
        "\n",
        "for file in os.listdir(\"../SVHN3_1/train/\"):\n",
        "  for n in os.listdir(\"../SVHN3_1/train/\"+file):\n",
        "    pathlist.append(\"../SVHN3_1/train/\"+file+\"/\"+n)\n",
        "    label.append(int(file))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sN7bkdPRYC8"
      },
      "source": [
        "len(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jalounBuRYAy"
      },
      "source": [
        "len(pathlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk9rv04pRX-p"
      },
      "source": [
        "data = {\n",
        "   'Paths': [],\n",
        "   'Labels': []\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Paths'] = pathlist\n",
        "df['Labels'] = label\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzzlK6AJU1xj"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcuWVIzhRX7e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv('../gdrive/My Drive/SVHN3_1_train.csv')\n",
        "df.to_csv('../SVHN3_1_train.csv')\n",
        "#files.download('../gdrive/My Drive/.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-xZtBtqs_HK"
      },
      "source": [
        "new_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPhrnX6s_HM"
      },
      "source": [
        "l = [\"Paths\", \"Labels\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9cPZiqGs_HN"
      },
      "source": [
        "new_dict = dict(zip(new_dict, l))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFkSeEoNs_HQ"
      },
      "source": [
        "new_dict.update(dict(zip(new_dict, l)))\n",
        "new_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF8wrkDDs_HR"
      },
      "source": [
        "pathlist = []\n",
        "label = []\n",
        "\n",
        "for file in os.listdir(\"../SVHN3_1/test/\"):\n",
        "  for n in os.listdir(\"../SVHN3_1/test/\"+file):\n",
        "    pathlist.append(\"../SVHN3_1/test/\"+file+\"/\"+n)\n",
        "    label.append(int(file))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B745h47s_HS"
      },
      "source": [
        "\n",
        "len(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9O4L2WDs_HT"
      },
      "source": [
        "len(pathlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXtg3UVRs_HU"
      },
      "source": [
        "data = {\n",
        "   'Paths': [],\n",
        "   'Labels': []\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Paths'] = pathlist\n",
        "df['Labels'] = label\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNo6ZkUds_HV"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGqLkL2vs_HV"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv('../gdrive/My Drive/SVHN3_1_test.csv')\n",
        "df.to_csv('../SVHN3_1_test.csv')\n",
        "#files.download('../gdrive/My Drive/.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKfOIE5nrMnl"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi8c7Cf7rMlA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GITRHd_Uk0N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA0Q7OLLzlpg"
      },
      "source": [
        "class CustomDatasetFromCsvData(Dataset):\n",
        "    def __init__(self, csv_path, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset example for reading data from csv\n",
        "        Args:\n",
        "            csv_path (string): path to csv file\n",
        "            height (int): image height\n",
        "            width (int): image width\n",
        "            transform: pytorch transforms for transforms and tensor conversion\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.paths = self.data['Paths']\n",
        "        self.labels = np.asarray(self.data['Labels'])\n",
        "        #self.height = height\n",
        "        #self.width = width\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        single_image_label = self.labels[index]\n",
        "        # Read each 784 pixels and reshape the 1D array ([784]) to 2D array ([28,28])\n",
        "        img_as_np = Image.open(self.paths[index])\n",
        "        img_as_np = np.reshape(img_as_np, (32, 32, 3))\n",
        "        #img_as_np = np.asarray(self.data.iloc[index][1:]).reshape(28, 28).astype('uint8')\n",
        "        # Convert image from numpy array to PIL image, mode 'L' is for grayscale\n",
        "        img_as_img = Image.fromarray(img_as_np)\n",
        "        #img_as_img = img_as_img.convert('L')\n",
        "        # Transform image to tensor\n",
        "        if self.transform is not None:\n",
        "            img_as_tensor = self.transform(img_as_img)\n",
        "        #img_as_tensor = Variable(img_as_tensor)\n",
        "        #single_image_label = Variable(torch.Tensor(single_image_label))\n",
        "        # Return image and the label\n",
        "        return (img_as_tensor, single_image_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Ym17NhNlB4"
      },
      "source": [
        "train_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06V661lQNDvg"
      },
      "source": [
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "\n",
        "PATH_TO_SVHN3 = '../SVHN3_1'\n",
        "\n",
        "trainset = datasets.ImageFolder(root=PATH_TO_SVHN3+'/train',transform=train_transform)\n",
        "testset = datasets.ImageFolder(root=PATH_TO_SVHN3+'/test',transform=test_transform)\n",
        "\n",
        "BATCH_SIZE=128\n",
        "\n",
        "SAMPLE_RATE = BATCH_SIZE / len(trainset)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_sampler=UniformWithReplacementSampler(\n",
        "        num_samples=len(trainset),\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "    ),\n",
        ")\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=128,\n",
        "    shuffle=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVqEtP091hvo"
      },
      "source": [
        "# from opacus.dp_model_inspector import DPModelInspector\n",
        "\n",
        "# inspector = DPModelInspector()\n",
        "# inspector.validate(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftcpb3S71zac"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWo1EWYCNDyU"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print(type(images))\n",
        "print(type(labels))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmIQFvZEN9tF"
      },
      "source": [
        "plt.imshow(images[10].permute(1,2,0).numpy());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQexw1WXN9v_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE3j6NnbkH94"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "class_freq_counter = collections.Counter(trainloader.dataset.targets)\n",
        "train_class_freq = [val for val in class_freq_counter.values()]\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "for idx, val in enumerate(train_class_freq):\n",
        "  plt.text((idx-0.25),val+40, str(val), fontsize=12)\n",
        "plt.bar(classes, train_class_freq, color='green')\n",
        "plt.title(\"SVHN3 Training Dataset Distribution\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni2FpTDXPxWw"
      },
      "source": [
        "train_class_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID0XKsH0PyEe"
      },
      "source": [
        "print(\"Total number of samples in Train dataset :\", np.sum(train_class_freq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMR3VC6IP_XG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDpxfKNKxIT1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "class_freq_counter = collections.Counter(testloader.dataset.targets)\n",
        "test_class_freq = [val for val in class_freq_counter.values()]\n",
        "\n",
        "plt.figure(figsize = (10,10))\n",
        "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "for idx, val in enumerate(test_class_freq):\n",
        "  plt.text((idx-0.25),val+10, str(val), fontsize=12)\n",
        "#plt.text(0,980, \"980\")\n",
        "#plt.text(1,1135, \"1135\")\n",
        "plt.bar(classes, test_class_freq, color='green')\n",
        "plt.title(\"SVHN3 Test Dataset Distribution\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBXsm9wZyzb_"
      },
      "source": [
        "test_class_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdxkrAlgyphe"
      },
      "source": [
        "print(\"Total number of samples in Train dataset :\", np.sum(test_class_freq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "817OvV-NQNMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX3ZRXa6jqk3"
      },
      "source": [
        "'''\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"Class used to initialize model of student/teacher\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4 * 4 * 50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        #return F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1imSMn-igzcX"
      },
      "source": [
        "\n",
        "\n",
        "# class Model(nn.Module):\n",
        "#     \"\"\"Class used to initialize model of student/teacher\"\"\"\n",
        "#     def __init__(self):\n",
        "#         super(Model, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 96, 3, 1)\n",
        "#         self.conv2 = nn.Conv2d(96, 96, 3, 1)\n",
        "#         self.conv3 = nn.Conv2d(96, 96, 3, 1)\n",
        "#         #self.bn1 = nn.BatchNorm2d(96)\n",
        "#         self.conv4 = nn.Conv2d(96, 192, 3, 1)\n",
        "#         self.conv5 = nn.Conv2d(192, 192, 3, 1)\n",
        "#         self.conv6 = nn.Conv2d(192, 192, 3, 1)\n",
        "#         self.conv7 = nn.Conv2d(192, 192, 5, 1)\n",
        "#         #self.bn2 = nn.BatchNorm2d(192)\n",
        "#         self.fc1 = nn.Linear(2 * 2 * 192, 500)\n",
        "#         self.fc2 = nn.Linear(500, 10)\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn1(x)\n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn1(x)        \n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         x = F.relu(self.conv3(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn1(x)\n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn2(x)\n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         x = F.relu(self.conv5(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn2(x)\n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         x = F.relu(self.conv6(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn2(x)\n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         x = F.relu(self.conv7(x))\n",
        "#         x = F.max_pool2d(x, 3, 1)\n",
        "#         #x = self.bn2(x)\n",
        "#         x = F.dropout(x, 0.5)\n",
        "#         # print(x.shape)\n",
        "#         x = x.view(-1, 2 * 2 * 192)\n",
        "#         # print(x.shape)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         #return F.log_softmax(x, dim=1)\n",
        "#         return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aagWQvDVoRhq"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNqLm0LE15Vl"
      },
      "source": [
        "model = ResNet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnhjmyryxq4C"
      },
      "source": [
        "summary(model.cuda(), (3,32,32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnoEyEN5gzfR"
      },
      "source": [
        "from opacus.dp_model_inspector import DPModelInspector\n",
        "\n",
        "inspector = DPModelInspector()\n",
        "inspector.validate(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncdkguE5QMDz"
      },
      "source": [
        "from opacus.dp_model_inspector import DPModelInspector\n",
        "from opacus.utils import module_modification\n",
        "\n",
        "model = module_modification.convert_batchnorm_modules(model)\n",
        "inspector = DPModelInspector()\n",
        "print(f\"Is the model valid? {inspector.validate(model)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT57Si5e6P5z"
      },
      "source": [
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLuJeOsijuE_"
      },
      "source": [
        "def accuracy(predictions, dataset):\n",
        "    \"\"\"Evaluates accuracy for given set of predictions and true labels.\n",
        "       Args:\n",
        "           predictions[torch tensor]: predictions made by classifier.\n",
        "           labels[torch tensor]: true labels of the dataset.\n",
        "       Returns:\n",
        "           accuracy[float]: accuracy of classifier.\n",
        "    \"\"\"\n",
        "\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    for j in range(0, len(dataset)):\n",
        "        correct += (predictions[j].cpu().long() == dataset[j].cpu().long()).sum().item()\n",
        "        total += len(dataset[j])\n",
        "\n",
        "    return (correct / total) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3HpxIHr-Y-"
      },
      "source": [
        "#model = Model().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T5YkLdpsHtq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFzfHjl1sIUd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlzhGf6fwthq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvUa37k7P-9S"
      },
      "source": [
        "### DP-SGD with eps = 5.06"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS3Ky2IowtkS"
      },
      "source": [
        "# batchsize = 256\n",
        "# from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "\n",
        "# #from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "\n",
        "\n",
        "# rdp = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=np.sum(train_class_freq), batch_size=batchsize, noise_multiplier=0.66, epochs=18, delta=1e-4)\n",
        "# #epsilon = get_privacy_spent(5218, rdp, target_delta=1e-5)[0]\n",
        "# rdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhPxeQ2SzxVb"
      },
      "source": [
        "# batchsize = 32\n",
        "# from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "\n",
        "# #from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n",
        "\n",
        "\n",
        "# rdp = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=np.sum(train_class_freq), batch_size=batchsize, noise_multiplier=0.51, epochs=18, delta=1e-4)\n",
        "# #epsilon = get_privacy_spent(5218, rdp, target_delta=1e-5)[0]\n",
        "# rdp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA96zTkPQMp5"
      },
      "source": [
        "# train_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "# test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGQjfgKT3OcQ"
      },
      "source": [
        "# PATH_TO_SVHN3 = '/content/SVHN3_1'\n",
        "\n",
        "# trainset = datasets.ImageFolder(root=PATH_TO_SVHN3+'/train',transform=train_transform)\n",
        "# testset = datasets.ImageFolder(root=PATH_TO_SVHN3+'/test',transform=test_transform)\n",
        "# train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcQOV0VjQGxX"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGDm22T25AC"
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1meCuE9c2-il"
      },
      "source": [
        "EPOCHS=50\n",
        "EPSILON=5\n",
        "MAX_GRAD_NORM = 1\n",
        "DELTA = 1e-5\n",
        "VIRTUAL_BATCH_SIZE=256\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy0ofQ-d27QF"
      },
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    epochs = EPOCHS,\n",
        "    target_epsilon = EPSILON,\n",
        "    #noise_multiplier = 0.8591845703125,\n",
        "    target_delta = DELTA,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)\n",
        "\n",
        "print(f\"Using sigma={privacy_engine.noise_multiplier} and C={MAX_GRAD_NORM}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWyaBY2s3YBO"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch, device):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    for i, (data, target) in enumerate(train_loader):  \n",
        "\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "        labels = target.detach().cpu().numpy()\n",
        "        \n",
        "        # measure accuracy and record loss\n",
        "        acc = accuracy(preds, labels)\n",
        "        losses.append(loss.item())\n",
        "        top1_acc.append(acc) \n",
        "        #print(loss)\n",
        "        #print(acc)       \n",
        "\n",
        "        loss.backward()\n",
        "        \t\n",
        "        # take a real optimizer step after N_VIRTUAL_STEP steps t\n",
        "        if ((i + 1) % N_ACCUMULATION_STEPS == 0) or ((i + 1) == len(train_loader)):\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step() # take a virtual step\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "            print(\n",
        "                f\"\\tTrain Epoch: {epoch} \\t\"\n",
        "                f\"Loss: {np.mean(losses):.6f} \"\n",
        "                f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
        "                f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msortXrh3X9r"
      },
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    losses = []\n",
        "    top1_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, target in test_loader:\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
        "            labels = target.detach().cpu().numpy()\n",
        "            acc = accuracy(preds, labels)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            top1_acc.append(acc)\n",
        "\n",
        "    top1_avg = np.mean(top1_acc)\n",
        "\n",
        "    print(\n",
        "        f\"\\tTest set:\"\n",
        "        f\"Loss: {np.mean(losses):.6f} \"\n",
        "        f\"Acc: {top1_avg * 100:.6f} \"\n",
        "    )\n",
        "    return np.mean(top1_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5SObKfR3pe_"
      },
      "source": [
        "def test1(model, testloader, optimizer):\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    #correct = 0\n",
        "    #total = 0\n",
        "\n",
        "    epoch_loss = 0.\n",
        "    epoch_acc = 0.\n",
        "    \n",
        "    batch_num = 0.\n",
        "    samples_num = 0.\n",
        "    \n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in  enumerate(testloader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            #_, predicted = torch.max(outputs.data, 1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            true_labels.append(labels.detach().cpu())\n",
        "            pred_labels.append(preds.detach().cpu())\n",
        "\n",
        "            # print(f'\\r{phase} batch [{batch_idx}/{len(testloader)}]: loss {loss}', end='', flush=True)\n",
        "            #print(f'\\r{phase} batch [{batch_idx}/{len(testloader)}]: loss {torch.mean(loss).item()}', end='', flush=True)\n",
        "            epoch_loss += torch.mean(loss.detach().cpu()).item()\n",
        "            epoch_acc += torch.sum(preds == labels.data)\n",
        "            batch_num += 1\n",
        "            samples_num += len(labels)\n",
        "\n",
        "            #total += labels.size(0)\n",
        "            #correct += (predicted == labels).sum().item()\n",
        "        #print(\"TESTING.....\")\n",
        "        return epoch_loss / batch_num, epoch_acc / samples_num, torch.cat(true_labels).numpy(), torch.cat(pred_labels).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvUD1v3-3rhA"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flyjmiaD328P"
      },
      "source": [
        "trainloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMyPBPic3Xz_"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "for i in range(1):\n",
        "\n",
        "  #model = ResNet18()\n",
        "\n",
        "  #model = model.cuda()\n",
        "\n",
        "  #model = module_modification.convert_batchnorm_modules(model)\n",
        "\n",
        "  for epoch in tqdm(range(EPOCHS)):\n",
        "    train(model, trainloader, optimizer, epoch + 1, device)\n",
        "\n",
        "\n",
        "\n",
        "  top1_acc = test(model, testloader, device)\n",
        "\n",
        "  Test_loss_after_training, test_acc_after_training, true_labels_test, pred_labels_test = test1(model, testloader, optimizer)\n",
        "\n",
        "\n",
        "  conf_mat_test =confusion_matrix(true_labels_test, pred_labels_test)\n",
        "  class_accuracy_test=100*conf_mat_test.diagonal()/conf_mat_test.sum(1)\n",
        "\n",
        "  print(\"Test results for run {}\".format(i+1))\n",
        "  for label, acc in enumerate(class_accuracy_test):\n",
        "    print(\"Test accuracy of class {} is {}\".format(str(label), str(round(acc, 2))+\"%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haR8CMdJ3Xwx"
      },
      "source": [
        "top1_acc = test(model, testloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbGtH6J3pcZ"
      },
      "source": [
        "Test_loss_after_training, test_acc_after_training, true_labels_test, pred_labels_test = test(model, testloader, optimizer)\n",
        "#test_nrun_true_labels.append(true_labels_test)\n",
        "#test_nrun_pred_labels.append(pred_labels_test)\n",
        "#test_nrun_acc.append(test_acc_after_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av8SetnVMWfr"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "conf_mat_test =confusion_matrix(true_labels_test, pred_labels_test)\n",
        "class_accuracy_test=100*conf_mat_test.diagonal()/conf_mat_test.sum(1)\n",
        "print(\"Test results for run 1\")\n",
        "print(class_accuracy_test)\n",
        "for label, acc in enumerate(class_accuracy_test):\n",
        "  print(\"Test accuracy of class {} is {}\".format(str(label), str(round(acc, 2))+\"%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCu6sDNw3pYz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0GnyL7EL090"
      },
      "source": [
        "# IGNORE BELOW CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwDZKbs5L0wX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25CFBeUTL0oL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCLsP8AWL0ff"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odfuFQVP3Oem"
      },
      "source": [
        "def clip_grad(parameters, max_norm, norm_type=2):\n",
        "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
        "    total_norm = 0\n",
        "    for p in parameters:\n",
        "        param_norm = p.grad.data.norm(norm_type)\n",
        "        total_norm += param_norm.item() ** norm_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FOrHWyC3SNc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PW98byX3SPz"
      },
      "source": [
        "def train_dp(model, trainloader, criterion, optimizer, lr_scheduler, phase='train'):\n",
        "    \"\"\"\n",
        "    Differentially Private version of the training procedure\n",
        "    :param trainloader:\n",
        "    :param model:\n",
        "    :param optimizer:\n",
        "    :param epoch:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = 0.\n",
        "    epoch_acc = 0.\n",
        "    \n",
        "    batch_num = 0.\n",
        "    samples_num = 0.\n",
        "    \n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    #num_microbatches = 32*2\n",
        "    num_microbatches = 32\n",
        "    S = 1\n",
        "    '''\n",
        "    z = 0.77  #DP-SGD with sampling rate = 0.543% and noise_multiplier = 0.77 iterated over 9201 steps satisfies differential privacy with eps = 5.06 and delta = 0.0001.\n",
        "              #The optimal RDP order is 4.0.\n",
        "    '''\n",
        "\n",
        "    z = 0.66  #DP-SGD with sampling rate = 0.543% and noise_multiplier = 0.66 iterated over 3313 steps satisfies differential privacy with eps = 4.99 and delta = 0.0001.\n",
        "              #The optimal RDP order is 3.5.\n",
        "    sigma = z * S\n",
        "    for batch_idx, data in tqdm(enumerate(trainloader, 0), leave=True):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model = model.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        print(loss)\n",
        "        print(loss.shape)\n",
        "        running_loss += torch.mean(loss).item()\n",
        "\n",
        "        true_labels.append(labels.detach().cpu())\n",
        "        pred_labels.append(preds.detach().cpu())\n",
        "\n",
        "        if loss.shape[0] == 3:\n",
        "          losses = torch.mean(loss.reshape(3, -1), dim=1)\n",
        "        else:\n",
        "          losses = torch.mean(loss.reshape(num_microbatches, -1), dim=1)\n",
        "        \n",
        "        saved_var = dict()\n",
        "        for tensor_name, tensor in model.named_parameters():\n",
        "            saved_var[tensor_name] = torch.zeros_like(tensor)\n",
        "\n",
        "        for j in losses:\n",
        "            j.backward(retain_graph=True)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), S)\n",
        "            for tensor_name, tensor in model.named_parameters():\n",
        "                new_grad = tensor.grad\n",
        "                saved_var[tensor_name].add_(new_grad)\n",
        "            model.zero_grad()\n",
        "\n",
        "        for tensor_name, tensor in model.named_parameters():\n",
        "            if device.type =='cuda':\n",
        "                noise = torch.cuda.FloatTensor(tensor.grad.shape).normal_(0, sigma)\n",
        "                #noise = torch.FloatTensor(tensor.grad.shape).normal_(0, sigma)\n",
        "            else:\n",
        "                noise = torch.FloatTensor(tensor.grad.shape).normal_(0, sigma)\n",
        "            saved_var[tensor_name].add_(noise)\n",
        "            tensor.grad = saved_var[tensor_name] / num_microbatches\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(f'\\r{phase} batch [{batch_idx}/{len(trainloader)}]: loss {loss}', end='', flush=True)\n",
        "        ##print(f'\\r{phase} batch [{batch_idx}/{len(trainloader)}]: loss {torch.mean(loss).item()}', end='', flush=True)\n",
        "        epoch_loss += torch.mean(loss.detach().cpu()).item()\n",
        "        epoch_acc += torch.sum(preds == labels.data)\n",
        "        batch_num += 1\n",
        "        samples_num += len(labels)\n",
        "\n",
        "        '''if i > 0 and i % 20 == 0:\n",
        "            #             logger.info('[%d, %5d] loss: %.3f' %\n",
        "            #                   (epoch + 1, i + 1, running_loss / 2000))\n",
        "            plot(epoch * len(trainloader) + i, running_loss, 'Train Loss')\n",
        "            running_loss = 0.0'''\n",
        "    #print(\"RETURNING........\")\n",
        "    return epoch_loss / batch_num, epoch_acc / samples_num, torch.cat(true_labels).numpy(), torch.cat(pred_labels).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEKZVWwJ3S5g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RfW3x5-3S7g"
      },
      "source": [
        "def test(model, testloader, criterion, optimizer, lr_scheduler, phase='test'):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    #correct = 0\n",
        "    #total = 0\n",
        "\n",
        "    epoch_loss = 0.\n",
        "    epoch_acc = 0.\n",
        "    \n",
        "    batch_num = 0.\n",
        "    samples_num = 0.\n",
        "    \n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in  enumerate(testloader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            #_, predicted = torch.max(outputs.data, 1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            true_labels.append(labels.detach().cpu())\n",
        "            pred_labels.append(preds.detach().cpu())\n",
        "\n",
        "            # print(f'\\r{phase} batch [{batch_idx}/{len(testloader)}]: loss {loss}', end='', flush=True)\n",
        "            #print(f'\\r{phase} batch [{batch_idx}/{len(testloader)}]: loss {torch.mean(loss).item()}', end='', flush=True)\n",
        "            epoch_loss += torch.mean(loss.detach().cpu()).item()\n",
        "            epoch_acc += torch.sum(preds == labels.data)\n",
        "            batch_num += 1\n",
        "            samples_num += len(labels)\n",
        "\n",
        "            #total += labels.size(0)\n",
        "            #correct += (predicted == labels).sum().item()\n",
        "        #print(\"TESTING.....\")\n",
        "        return epoch_loss / batch_num, epoch_acc / samples_num, torch.cat(true_labels).numpy(), torch.cat(pred_labels).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJgHX-m33Zcp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUC5QRT3cpF"
      },
      "source": [
        "criterion=nn.CrossEntropyLoss(reduction=\"none\")\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.001, momentum=0.5)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.5)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfysDNqRrgB4"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "train_nrun_losses = []\n",
        "val_nrun_losses = []\n",
        "test_nrun_losses = []\n",
        "test_nrun_true_labels = []\n",
        "test_nrun_pred_labels = []\n",
        "test_nrun_acc = []\n",
        "\n",
        "def train_n(runs, nepochs = 50):\n",
        "  for run in range(runs):\n",
        "    model = Model().cuda()\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.001)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
        "    print(\"Train Log of run \", run)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    best_model = None\n",
        "    for epoch in range(nepochs):\n",
        "        print('='*15, f'Epoch: {epoch}')\n",
        "    \n",
        "        #train_dp(model, trainloader, criterion, optimizer, lr_scheduler, phase='train')\n",
        "        #test(model, testloader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "\n",
        "        train_loss, train_acc, true_labels_train, pred_labels_train = train_dp(model, train_loader, criterion, optimizer, lr_scheduler)\n",
        "        #val_loss, val_acc, _, _ = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='val')\n",
        "        test_loss, test_acc, _, _ = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "        #test_loss, test_acc, true_labels, pred_labels = run_epoch(model, test_dataloader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "    \n",
        "        print()\n",
        "        print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
        "        #print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')\n",
        "        print(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n",
        "        print()\n",
        "    \n",
        "        ##train_losses.append(train_loss)\n",
        "        ##train_accuracies.append(train_acc)\n",
        "        #val_losses.append(test_loss)\n",
        "        #test_losses.append(test_loss)\n",
        "        ##test_accuracies.append(test_acc)\n",
        "\n",
        "\n",
        "        \n",
        "        conf_mat_train =confusion_matrix(true_labels_train, pred_labels_train)\n",
        "        class_accuracy_train=100*conf_mat_train.diagonal()/conf_mat_train.sum(1)\n",
        "        print(\"Train results for epoch \", epoch)\n",
        "        for label_t, acc_t in enumerate(class_accuracy_train):\n",
        "          print(\"Train accuracy of class {} is {}\".format(str(label_t), str(round(acc_t, 2))+\"%\"))\n",
        "\n",
        "        Test_loss_after_training, test_acc_after_training, true_labels_test, pred_labels_test = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "        conf_mat_test =confusion_matrix(true_labels_test, pred_labels_test)\n",
        "        class_accuracy_test=100*conf_mat_test.diagonal()/conf_mat_test.sum(1)\n",
        "        print(\"Test results for epoch \", epoch)\n",
        "        for label, acc in enumerate(class_accuracy_test):\n",
        "          print(\"Test accuracy of class {} is {}\".format(str(label), str(round(acc, 2))+\"%\"))\n",
        "    \n",
        "        torch.save({'epoch': epoch, 'model': model.state_dict()}, f'covnet-{seed}.pt')\n",
        "    \n",
        "        '''\n",
        "        if best_model is None or test_loss < best_loss:\n",
        "            best_model = copy.deepcopy(model)\n",
        "            #best_loss = val_loss\n",
        "            best_test_loss = test_loss\n",
        "            best_test_acc = test_acc \n",
        "            best_pred_labels = pred_labels\n",
        "            torch.save({'epoch': epoch, 'model': model.state_dict()}, f'covnet-{seed}.pt')\n",
        "        '''\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_loss, test_acc, _, _ = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    train_nrun_losses.append(train_losses)\n",
        "    test_nrun_losses.append(test_losses)\n",
        "    Test_loss_after_training, test_acc_after_training, true_labels_test, pred_labels_test = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "    test_nrun_true_labels.append(true_labels_test)\n",
        "    test_nrun_pred_labels.append(pred_labels_test)\n",
        "    test_nrun_acc.append(test_acc_after_training)\n",
        "    conf_mat_test =confusion_matrix(true_labels_test, pred_labels_test)\n",
        "    class_accuracy_test=100*conf_mat_test.diagonal()/conf_mat_test.sum(1)\n",
        "    print(\"Test results for run \", run)\n",
        "    for label, acc in enumerate(class_accuracy_test):\n",
        "      print(\"Test accuracy of class {} is {}\".format(str(label), str(round(acc, 2))+\"%\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-SfZ0zQtK0_"
      },
      "source": [
        "train_n(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYkkOjz3tmyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygeX4Nd06bzW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nynxzg1m6b2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOfcKdO3Ze9"
      },
      "source": [
        "'''\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(50):\n",
        "    print('='*15, f'Epoch: {epoch}')\n",
        "    \n",
        "#train_dp(model, trainloader, criterion, optimizer, lr_scheduler, phase='train')\n",
        "#test(model, testloader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "\n",
        "    train_loss, train_acc, _, _ = train_dp(model, train_loader, criterion, optimizer, lr_scheduler)\n",
        "    #val_loss, val_acc, _, _ = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='val')\n",
        "    test_loss, test_acc, _, _ = test(model, test_loader, criterion, optimizer, lr_scheduler, phase='val') ### IMPORTANT :::: Although this is test_dataloader, the name for phase is given as 'val', it is just a incorrect label and nothing else\n",
        "    #test_loss, test_acc, true_labels, pred_labels = run_epoch(model, test_dataloader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "    \n",
        "    print()\n",
        "    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
        "    #print(f'Val loss: {val_loss}, Val accuracy: {val_acc}')\n",
        "    print(f'Test loss: {test_loss}, Test accuracy: {test_acc}')\n",
        "    print()\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    #val_losses.append(test_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    \n",
        "    torch.save({'epoch': epoch, 'model': model.state_dict()}, f'covnet-{seed}.pt')\n",
        "    \n",
        "    if best_model is None or test_loss < best_loss:\n",
        "        best_model = copy.deepcopy(model)\n",
        "        #best_loss = val_loss\n",
        "        best_test_loss = test_loss\n",
        "        best_test_acc = test_acc \n",
        "        best_pred_labels = pred_labels\n",
        "        torch.save({'epoch': epoch, 'model': model.state_dict()}, f'covnet-{seed}.pt')\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4bcUy2K5D9o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3uNfHls_X-F"
      },
      "source": [
        "'''\n",
        "train_after_loss, train_after_acc, true_labels_train, pred_labels_train = test(model, train_loader, criterion, optimizer, lr_scheduler, phase='test')\n",
        "print(f'Train loss (After Training is Completed): {train_after_loss}, Train accuracy (After Training is Completed): {train_after_acc}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXtIBpmp_YAX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1l-m5856nsk"
      },
      "source": [
        "'''\n",
        "test_nrun_true_labels = []\n",
        "test_nrun_pred_labels = []\n",
        "test_nrun_acc = []\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPSvPQsh7A0G"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "runs = 5\n",
        "class_accuracy_nrun_test = []\n",
        "for run in range(runs):\n",
        "  conf_mat_test =confusion_matrix(test_nrun_true_labels[run], test_nrun_pred_labels[run])\n",
        "  class_accuracy_test=100*conf_mat_test.diagonal()/conf_mat_test.sum(1)\n",
        "  #print(class_accuracy_test)\n",
        "  class_accuracy_nrun_test.append(class_accuracy_test)\n",
        "\n",
        "mean_class_acc_test = np.mean(np.array(class_accuracy_nrun_test), axis=0)\n",
        "std_class_acc_test = np.std(np.array(class_accuracy_nrun_test), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFvta6mb8Qh6"
      },
      "source": [
        "for label, acc in enumerate(mean_class_acc_test):\n",
        "  print(\"Average Test accuracy of class {} is {}\".format(str(label), str(round(acc, 2))+\"%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAw8q-cF8Qkp"
      },
      "source": [
        "for label, std_acc in enumerate(std_class_acc_test):\n",
        "  print(\"Standard Deviation of Test accuracy of class {} is {}\".format(str(label), str(round(std_acc, 2))+\"%\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT1IBzwT82Pe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8q6ptfc9cv0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10,10))\n",
        "\n",
        "ax.bar(labels, mean_class_acc_test, width, yerr=std_class_acc_test, label='SVHN Digits(Test)')\n",
        "\n",
        "ax.set_ylabel('Class-Wise Accuracies')\n",
        "ax.set_title('Class-Wise Test Accuracies for 5 runs (EPS = 5.0)')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-hQFXUc9cyN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWDy8vKF82R1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}